{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deep Dive in Pytorch\n",
    "- 파이토치 사용법을 좀 더 고급화 하기 위해, 책 한 권 [딥러닝 파이토치 교과서]을 정해, 정리해보려고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Tensor\n",
    "- Pytorch는 텐서에 대한 제어를 확실히 해야함.\n",
    "- 개인적으로, Pytorch와 Tensorflow를 병행해서 사용하다보니, 때때로 많이 헷갈리는 경우가 생김."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 텐서 생성 및 변환"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(torch\u001B[38;5;241m.\u001B[39mtensor([[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m], [\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m]]))\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcuda:0\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(torch\u001B[38;5;241m.\u001B[39mtensor([[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m], [\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m]], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat64))\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/cuda/__init__.py:211\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    208\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    209\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m--> 211\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    212\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    213\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    214\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# Tensor 생성\n",
    "import torch\n",
    "print(torch.tensor([[1, 2], [3, 4]]))\n",
    "print(torch.tensor([[1, 2], [3, 4]], device=\"cuda:0\"))\n",
    "print(torch.tensor([[1, 2], [3, 4]], dtype=torch.float64))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "# Tensor -> numpy array\n",
    "tensor_a = torch.tensor([[1, 2], [3, 4]])\n",
    "print(tensor_a.numpy())\n",
    "\n",
    "tensor_gpu = torch.tensor([[1, 2], [3, 4]], device=\"cuda:0\")\n",
    "print(tensor_gpu.to(\"cpu\").numpy())\n",
    "\n",
    "# GPU 메모리에 올라가 있는 Tensor는 바로 numpy() 변환 되지 않음\n",
    "# print(tensor_gpu.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 텐서의 자료형 및 인덱스 조작\n",
    "**Tensor 자료형**\n",
    "1. torch.FloatTensor : 32비트 부동 소수점 텐서\n",
    "2. torch.DoubleTensor : 64비트 부동 소수점 텐서\n",
    "3. torch.LongTensor : 64비트 부호가 있는 정수"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(2.) tensor(3.)\n",
      "tensor([1., 2., 3.]) tensor([4., 5.]) tensor([4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "float_tensor = torch.FloatTensor([1, 2, 3, 4, 5, 6, 7])\n",
    "print(float_tensor[0], float_tensor[1], float_tensor[2])\n",
    "print(float_tensor[0:3], float_tensor[3:5], float_tensor[-4:-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  텐서 연산\n",
    "- 텐서간 연산은 텐서의 타입이 다르면 연산이 불가능\n",
    "- FloatTensor와 DoubleTensor 간에 사칙 연산을 하면 에러 발생\n",
    "    - pytorch 1.11 에서는 연산 가능"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([4, 5, 6])\n",
      "tensor([5., 7., 9.])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.Tensor([1, 2, 3])\n",
    "print(tensor_a)\n",
    "tensor_b = torch.tensor([4, 5, 6])\n",
    "print(tensor_b)\n",
    "\n",
    "print(tensor_a + tensor_b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 6., 9.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "sample_float = torch.FloatTensor([2, 4, 6])\n",
    "sample_double = torch.DoubleTensor([1, 2, 3])\n",
    "\n",
    "print(sample_float + sample_double)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2., 4., 6.])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_float"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1., 2., 3.], dtype=torch.float64)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_double"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 텐서 차원 조작\n",
    "- 텐서 차원 조작 방법 : ```view```\n",
    "    - numpy의 ```reshape```과 유사\n",
    "- 텐서 결합 : ```stack```, ```cat```\n",
    "    - ```cat``` : 다른 길이의 텐서를 하나로 병합할 때 사용\n",
    "- 텐서 차원 교환 : ```t```, ```transpose```\n",
    "    - ```transpose``` : 행렬 전치 외에도, 차원의 순서 변경 때도 사용\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "---\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n",
      "---\n",
      "tensor([1, 2, 3, 4])\n",
      "---\n",
      "tensor([[1, 2, 3, 4]])\n",
      "---\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n"
     ]
    }
   ],
   "source": [
    "tensor_sample = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "print(tensor_sample.shape)\n",
    "print(\"---\")\n",
    "print(tensor_sample.view(4, 1))\n",
    "print(\"---\")\n",
    "print(tensor_sample.view(-1))\n",
    "print(\"---\")\n",
    "print(tensor_sample.view(1, -1))\n",
    "print(\"---\")\n",
    "print(tensor_sample.view(-1, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Data Preparation\n",
    "- Pytorch Data 호출 방법\n",
    "    1. 파이썬 라이브러리 (판다스(Pandas))\n",
    "    2. Pytorch 제공 데이터\n",
    "- 데이터 타입에 따른 호출 방법\n",
    "    1. 이미지\n",
    "        - 분산된 파일에서 데이터 읽기\n",
    "        - 전처리\n",
    "        - 배치 단위로 분할 처리\n",
    "    2. 텍스트\n",
    "        - 임베딩 과정\n",
    "        - 서로 다른 길이의 Sequence를 배치 단위로 분할 처리"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 판다스를 사용한 데이터 임포트"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "tensor([[ 8.],\n",
      "        [ 9.],\n",
      "        [10.]])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/sample_01.csv')\n",
    "\n",
    "x = torch.from_numpy(data['x'].values).unsqueeze(dim=1).float()\n",
    "y = torch.from_numpy(data['y'].values).unsqueeze(dim=1).float()\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 커스텀 데이터셋을 만들어서 사용\n",
    "- 딥러닝은 대량의 데이터를 이용해 모델 학습을 시킴\n",
    "- 데이터를 한번에 메모리에 불러와서 훈련시키면 시간과 비용측면에서 비효율\n",
    "- 데이터를 한번에 다 부르지 않고, 커스텀 데이터셋을 이용\n",
    "\n",
    "**CustomDataset 구현**\n",
    "```\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        # 필요한 변수 선언, 데이터셋 전처리 함수\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋 길이, 총 샘플의 수를 가져오는 함수\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 데이터셋에서 특정 데이터를 가져오는 함수 (index 번째 데이터를 반환 하는 함수)\n",
    "        # 반환 값은 텐서 형태\n",
    "```\n",
    "\n",
    "**DataLoader**\n",
    "- 데이터로더 (DataLoader) 객체는 학습에 사용될 데이터 전체를 보관.\n",
    "- 모델 학스비, 배치 크기만큼 데이터를 꺼내서 사용\n",
    "- DataLoader는 데이터를 미리 잘라 놓는 것이 아니라, iterator를 활용해 index를 이용하여 배치 크기만큼 데이터를 반환"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.label = pd.read_csv(csv_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = torch.tensor(self.label.iloc[index, 0:3]).int()\n",
    "        label = torch.tensor(self.label.iloc[index, 3]).int()\n",
    "        return sample, label\n",
    "\n",
    "tensor_dataset = CustomDataset('./data/sample_01.csv')\n",
    "dataset = DataLoader(tensor_dataset, batch_size=4, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST_DATASET/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST_DATASET/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST_DATASET/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST_DATASET/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST_DATASET/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST_DATASET/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST_DATASET/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST_DATASET/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST_DATASET/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST_DATASET/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST_DATASET/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST_DATASET/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (1.0,))\n",
    "])\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "import requests\n",
    "download_root = './data/MNIST_DATASET'\n",
    "\n",
    "train_dataset = MNIST(download_root, transform=mnist_transform, train=True, download=True)\n",
    "valid_dataset = MNIST(download_root, transform=mnist_transform, train=False, download=True)\n",
    "test_dataset = MNIST(download_root, transform=mnist_transform, train=False, download=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 정의\n",
    "- 모델 정의 : module 을 상속한 클래스 사용\n",
    "    - 계층 (layer) : 모듈 또는 모듈을 구성하는 한개의 계층. convolution layer, linear layer 등\n",
    "    - 모듈 (module) : 한 개 이상의 계층이 모여서 구성. 모듈이 모여 새로운 모듈을 만들 수도 있음\n",
    "    - 모델 (model) : 최종적으로 원하는 네트워크. 한 개의 모듈이 모델이 될 수도 있음"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 단순 신경망 정의\n",
    "- ```nn.Module```을 상속받지 않는 단순 모델"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Linear(in_features=1, out_features=1, bias=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### nn.Module()을 상속하여 정의하는 방법\n",
    "- ```nn.Module```을 상속 시,\n",
    "    - ```__init()``` : 모델에서 사용될 모듈 (nn.Linear, nn.Conv2d 등), 활성화 함수 등을 정의\n",
    "    - ```forward()``` : 모델에서 실행되어야 하는 연산 정의"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer = Linear(inputs, 1)\n",
    "        self.activation = Sigmoid()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.layer(X)\n",
    "        X = self.activation(X)\n",
    "        return X\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}